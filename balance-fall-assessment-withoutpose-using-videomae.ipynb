{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6170939,"sourceType":"datasetVersion","datasetId":3540726}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abdullah892/balance-fall-assessment-withoutpose-using-videomae?scriptVersionId=270191682\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torch torchvision torchaudio  # If not installed; use --extra-index-url https://download.pytorch.org/whl/cu121 for CUDA\n!pip install transformers datasets av evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:33:25.750294Z","iopub.execute_input":"2025-10-22T08:33:25.750515Z","iopub.status.idle":"2025-10-22T08:34:53.525854Z","shell.execute_reply.started":"2025-10-22T08:33:25.750492Z","shell.execute_reply":"2025-10-22T08:34:53.524875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all logs, 1 = info, 2 = warnings, 3 = errors only\nimport warnings\nwarnings.filterwarnings('ignore')  # Suppress Python warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:34:53.527719Z","iopub.execute_input":"2025-10-22T08:34:53.527965Z","iopub.status.idle":"2025-10-22T08:34:53.532269Z","shell.execute_reply.started":"2025-10-22T08:34:53.527945Z","shell.execute_reply":"2025-10-22T08:34:53.531506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport av\nimport numpy as np\nfrom pathlib import Path\nfrom transformers import AutoImageProcessor, VideoMAEForVideoClassification, TrainingArguments, Trainer\nfrom datasets import Dataset, Features, ClassLabel, Array4D\nimport evaluate\nimport torch\nfrom tqdm import tqdm  # For progress bars if needed\n\nnp.random.seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:34:53.53289Z","iopub.execute_input":"2025-10-22T08:34:53.533153Z","iopub.status.idle":"2025-10-22T08:35:24.334681Z","shell.execute_reply.started":"2025-10-22T08:34:53.533133Z","shell.execute_reply":"2025-10-22T08:35:24.334065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\ndata_dir = Path('/kaggle/input/multiple-cameras-fall-dataset/')\nfor root, dirs, files in os.walk(data_dir):\n    print(f\"Root: {root}\")\n    print(f\"  Dirs: {dirs[:5]}...\")  # First few subdirs\n    print(f\"  Files: {files[:5]}...\")  # First few files\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:35:24.335402Z","iopub.execute_input":"2025-10-22T08:35:24.335898Z","iopub.status.idle":"2025-10-22T08:35:24.441321Z","shell.execute_reply.started":"2025-10-22T08:35:24.335877Z","shell.execute_reply":"2025-10-22T08:35:24.440521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import av\nimport numpy as np\nfrom pathlib import Path\nfrom transformers import AutoImageProcessor\nfrom datasets import Dataset, Features, Value, ClassLabel, Array4D\n\nnp.random.seed(0)\n\n# Frame sampling functions\ndef read_video_pyav(container, indices):\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    converted_len = int(clip_len * frame_sample_rate)\n    if seg_len < converted_len:\n        indices = np.linspace(0, seg_len - 1, num=clip_len).astype(np.int64)\n    else:\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices\n\n# Preprocessor\nimage_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n\n# Dataset loading\ndata_dir = Path('/kaggle/input/multiple-cameras-fall-dataset/dataset/dataset/')\n\n# Collect paths and labels\nvideo_paths = []\nlabels = []\n\nprint(\"Collecting video paths...\")\nfor folder in sorted(data_dir.iterdir()):\n    if folder.is_dir() and 'chute' in folder.name.lower():\n        scenario_num = int(folder.name.lower().replace('chute', ''))\n        label = 1 if scenario_num <= 22 else 0\n        for video in folder.glob('cam*.avi'):\n            video_paths.append(str(video))\n            labels.append(label)\n\nprint(f\"Total videos: {len(video_paths)} (Falls: {sum(labels)}, No-falls: {len(labels) - sum(labels)})\")\n\n# Process all videos\nprint(\"\\nProcessing all videos (this may take 5-15 minutes)...\")\nprocessed_videos = []\nprocessed_labels = []\nfailed_count = 0\n\nfor idx, (video_path, label) in enumerate(zip(video_paths, labels)):\n    try:\n        if idx % 10 == 0:\n            print(f\"Progress: {idx}/{len(video_paths)} videos...\")\n        \n        container = av.open(video_path)\n        video_stream = container.streams.video[0]\n        total_frames = video_stream.frames\n        \n        if total_frames == 0:\n            total_frames = sum(1 for _ in container.decode(video=0))\n            container.seek(0)\n        \n        indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=total_frames)\n        video = read_video_pyav(container, indices)\n        inputs = image_processor(list(video), return_tensors=\"pt\")\n        pixel_values = inputs['pixel_values'].squeeze(0).numpy()\n        \n        # Ensure it's a numpy array\n        processed_videos.append(pixel_values)\n        processed_labels.append(label)\n        \n        container.close()\n        \n    except Exception as e:\n        print(f\"Failed to process {video_path}: {e}\")\n        failed_count += 1\n\nprint(f\"\\n✓ Processing complete!\")\nprint(f\"Successfully processed: {len(processed_videos)}/{len(video_paths)}\")\nprint(f\"Failed: {failed_count}\")\n\n# Convert to numpy array first\nprint(\"\\nConverting to numpy arrays...\")\nprocessed_videos = np.array(processed_videos)\nprocessed_labels = np.array(processed_labels)\n\nprint(f\"Videos array shape: {processed_videos.shape}\")\nprint(f\"Labels array shape: {processed_labels.shape}\")\n\n# Create dataset with processed videos\nprint(\"\\nCreating HuggingFace dataset...\")\ndataset = Dataset.from_dict({\n    \"pixel_values\": processed_videos,\n    \"label\": processed_labels\n})\n\n# Cast to proper feature types\ndataset = dataset.cast_column(\"label\", ClassLabel(num_classes=2, names=['no_fall', 'fall']))\n\nprint(f\"\\n✓ Dataset created successfully!\")\nprint(f\"Columns: {dataset.column_names}\")\nprint(f\"Size: {len(dataset)}\")\nprint(f\"Sample pixel_values shape: {np.array(dataset[0]['pixel_values']).shape}\")\nprint(f\"Sample label: {dataset[0]['label']}\")\n\n# Split dataset\nprint(\"\\nSplitting dataset...\")\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"PREPROCESSING COMPLETE!\")\nprint(f\"{'='*60}\")\nprint(f\"Train size: {len(dataset['train'])}\")\nprint(f\"Test size: {len(dataset['test'])}\")\nprint(f\"Columns: {dataset['train'].column_names}\")\nprint(f\"Train pixel_values shape: {np.array(dataset['train'][0]['pixel_values']).shape}\")\nprint(f\"{'='*60}\\n\")\nprint(\"✓ Ready for training!\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:35:24.442163Z","iopub.execute_input":"2025-10-22T08:35:24.442387Z","iopub.status.idle":"2025-10-22T08:37:43.991316Z","shell.execute_reply.started":"2025-10-22T08:35:24.442371Z","shell.execute_reply":"2025-10-22T08:37:43.990629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport transformers\nfrom transformers import VideoMAEForVideoClassification, TrainingArguments, Trainer\nimport evaluate\n\n# Disable wandb\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Check environment\nprint(\"=\"*60)\nprint(\"TRAINING SETUP\")\nprint(\"=\"*60)\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"GPU Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\nprint(\"Transformers Version:\", transformers.__version__)\n\n# Verify dataset is ready\nprint(\"\\nDataset verification:\")\nprint(\"Train size:\", len(dataset['train']))\nprint(\"Test size:\", len(dataset['test']))\nprint(\"Train columns:\", dataset['train'].column_names)\n\n# Set format to PyTorch\ndataset.set_format(type='torch', columns=['pixel_values', 'label'])\n\nprint(\"Sample pixel_values shape:\", dataset['train'][0]['pixel_values'].shape)\nprint(\"Sample label:\", dataset['train'][0]['label'])\nprint(\"=\"*60 + \"\\n\")\n\n# Load model\nprint(\"Loading VideoMAE model...\")\nmodel = VideoMAEForVideoClassification.from_pretrained(\n    \"MCG-NJU/videomae-base-finetuned-kinetics\",\n    num_labels=2,\n    id2label={0: \"no_fall\", 1: \"fall\"},\n    label2id={\"no_fall\": 0, \"fall\": 1},\n    ignore_mismatched_sizes=True\n)\n\n# Move to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(f\"✓ Model loaded and moved to {device}\\n\")\n\n# Class weights for imbalance (10x weight for fall class)\nclass_weights = torch.tensor([1.0, 10.0]).to(device)\nprint(f\"Class weights: {class_weights.cpu().numpy()}\")\n\n# Custom Trainer for class weights\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = torch.nn.functional.cross_entropy(logits, labels, weight=class_weights)\n        return (loss, outputs) if return_outputs else loss\n\n# Data collator to rename 'label' to 'labels'\ndef collate_fn(batch):\n    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n    labels = torch.tensor([item['label'] for item in batch])\n    return {'pixel_values': pixel_values, 'labels': labels}\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/videomae-fall-detection\",\n    num_train_epochs=10,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    warmup_steps=100,\n    weight_decay=0.01,\n    learning_rate=5e-5,\n    logging_dir=\"/kaggle/working/logs\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    fp16=True if torch.cuda.is_available() else False,\n    dataloader_num_workers=0,\n    report_to=\"none\",\n    logging_steps=5,\n    logging_first_step=True,\n    gradient_accumulation_steps=2,\n    save_total_limit=2,  # Keep only 2 best checkpoints\n)\n\nprint(\"\\nTraining configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  FP16: {training_args.fp16}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n\n# Metrics\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions = torch.argmax(torch.tensor(eval_pred.predictions), dim=-1)\n    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n\n# Clear previous outputs\nprint(\"\\nCleaning previous outputs...\")\n!rm -rf /kaggle/working/videomae-fall-detection/*\n!rm -rf /kaggle/working/logs/*\n\n# Check GPU memory\nprint(\"\\nGPU Status:\")\n!nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n\n# Initialize Trainer\nprint(\"\\nInitializing Trainer...\")\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['test'],\n    compute_metrics=compute_metrics,\n    tokenizer=image_processor,\n    data_collator=collate_fn,\n)\n\nprint(\"✓ Trainer initialized\\n\")\n\n# Train\nprint(\"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\n\ntry:\n    trainer.train()\n    print(\"\\n\" + \"=\"*60)\n    print(\"✓ TRAINING COMPLETED SUCCESSFULLY!\")\n    print(\"=\"*60)\nexcept Exception as e:\n    print(\"\\n\" + \"=\"*60)\n    print(\"✗ TRAINING FAILED\")\n    print(\"=\"*60)\n    print(f\"Error: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise\n\n# Save final model\nprint(\"\\nSaving model...\")\ntrainer.save_model(\"/kaggle/working/videomae-fall-detection\")\nimage_processor.save_pretrained(\"/kaggle/working/videomae-fall-detection\")\n\nprint(\"\\nSaved files:\", os.listdir(\"/kaggle/working/videomae-fall-detection\"))\n\n# Final evaluation\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL EVALUATION\")\nprint(\"=\"*60)\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"{key}: {value}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:37:43.992231Z","iopub.execute_input":"2025-10-22T08:37:43.9925Z","iopub.status.idle":"2025-10-22T08:50:37.934176Z","shell.execute_reply.started":"2025-10-22T08:37:43.992474Z","shell.execute_reply":"2025-10-22T08:50:37.933458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nprint(\"Evaluating on entire test set...\")\n\n# Get all predictions\nall_predictions = []\nall_labels = []\nall_probs = []\n\nfor idx in range(len(dataset['test'])):\n    # Get the original video path (you'll need to match indices)\n    pixel_values = dataset['test'][idx]['pixel_values']\n    true_label = dataset['test'][idx]['label']\n    \n    # Predict\n    inputs = {'pixel_values': pixel_values.unsqueeze(0).to(device)}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n        predicted_class = torch.argmax(probabilities, dim=-1).item()\n    \n    all_predictions.append(predicted_class)\n    all_labels.append(true_label)\n    all_probs.append(probabilities[0][1].item())  # Fall probability\n\n# Classification report\nprint(\"\\n\" + \"=\"*60)\nprint(\"CLASSIFICATION REPORT\")\nprint(\"=\"*60)\nprint(classification_report(all_labels, all_predictions, \n                          target_names=['no_fall', 'fall']))\n\n# Confusion matrix\ncm = confusion_matrix(all_labels, all_predictions)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['No Fall', 'Fall'],\n            yticklabels=['No Fall', 'Fall'])\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.tight_layout()\nplt.savefig('/kaggle/working/confusion_matrix.png', dpi=300, bbox_inches='tight')\nprint(\"✓ Confusion matrix saved to /kaggle/working/confusion_matrix.png\")\nplt.show()\n\n# Calculate additional metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\naccuracy = accuracy_score(all_labels, all_predictions)\nprecision = precision_score(all_labels, all_predictions)\nrecall = recall_score(all_labels, all_predictions)\nf1 = f1_score(all_labels, all_predictions)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DETAILED METRICS\")\nprint(\"=\"*60)\nprint(f\"Accuracy:  {accuracy:.2%}\")\nprint(f\"Precision: {precision:.2%} (of predicted falls, how many are correct)\")\nprint(f\"Recall:    {recall:.2%} (of actual falls, how many detected)\")\nprint(f\"F1-Score:  {f1:.2%}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:59:22.365933Z","iopub.execute_input":"2025-10-22T08:59:22.366453Z","iopub.status.idle":"2025-10-22T08:59:41.196272Z","shell.execute_reply.started":"2025-10-22T08:59:22.366427Z","shell.execute_reply":"2025-10-22T08:59:41.195629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}